{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this hands-on lab we will perform few activities related to Sequence Models - in special Recurrent Neural Networks applied to sentiment analysis and time series forecasting.\n",
    "\n",
    "To perform those activities it is important to address some requirements beforehand:\n",
    "\n",
    "1) deploy one AWS EC2 instance (P2.8x type) to be used as sandbox (it could be destroyed after the lab execution)\n",
    "\n",
    "2) After logging in the instance, run 'source activate tensorflow_p36'\n",
    "\n",
    "3) Create a directory as 'mkdir -p /models/ai-conference' and enter on it 'cd /models/ai-conference'\n",
    "\n",
    "4) Clone the github repository containing the labs 'git clone github link'\n",
    "\n",
    "This notebook includes the following activities:\n",
    "\n",
    "- building a first sample RNN (LSTM) on NLP\n",
    "- train the neural network using the IMDB reviews dataset and evaluate its performance\n",
    "- report the performance metrics for that model, including precision, recall, f1score and support\n",
    "- performing transfer learning to speed up model creation process\n",
    "- building a second sample RNN (LSTM) network on Time Series forecasting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I - Sequence Models basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate that the required python modules are installed before starting\n",
    "\n",
    "!conda install -y seaborn Pillow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required modules\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from operator import itemgetter    \n",
    "from keras import models, regularizers, layers, optimizers, losses, metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.datasets import imdb\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Recurrent means the output at the current time step becomes the input to the next time step. At each element of the sequence, the model considers not just the current input, but what it remembers about the preceding elements.\"\n",
    "\n",
    "![Recurrent Neural Network](https://cdn-images-1.medium.com/max/1600/1*KljWrINqItHR6ng05ASR8w.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "# model configuration -- number of GPUs and training option (Yes or No)\n",
    "\n",
    "n_gpus = 8 # knob to make the model parallel or not\n",
    "train_model = False # knob to decide if the model will be trained or imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading IMDB dataset\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a look on the dataset info\n",
    "\n",
    "print(\"_\"*50)\n",
    "print(\"\\ntrain_data \", train_data.shape)\n",
    "print(\"train_labels \", train_labels.shape)\n",
    "print(\"_\"*50)\n",
    "print(\"\\ntest_data \", test_data.shape)\n",
    "print(\"test_labels \", test_labels.shape)\n",
    "print(\"_\"*50)\n",
    "print(\"\\nMaximum value of a word index \")\n",
    "print(max([max(sequence) for sequence in train_data]))\n",
    "print(\"\\nMaximum length num words of review in train \")\n",
    "print(max([len(sequence) for sequence in train_data]))\n",
    "print(\"_\"*50)\n",
    "\n",
    "# checking a sample from the dataset\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[123]])\n",
    "print('decoded text:\\n\\n', decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Vectorization](https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/assets/atap_0402.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to vectorize the dataset information\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "    \n",
    "# vectorizing the datasets\n",
    "\n",
    "X_train = vectorize_sequences(train_data)\n",
    "X_test = vectorize_sequences(test_data)\n",
    "\n",
    "print(\"x_train \", X_train.shape)\n",
    "print(\"x_test \", X_test.shape)\n",
    "\n",
    "# vectorizing the labels\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "print(\"y_train \", y_train.shape)\n",
    "print(\"y_test \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a validation set\n",
    "\n",
    "X_val = X_train[:10000]\n",
    "X_train = X_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "y_train = y_train[10000:]\n",
    "\n",
    "print(\"x_val \", X_val.shape)\n",
    "print(\"X_train \", X_train.shape)\n",
    "print(\"y_val \", y_val.shape)\n",
    "print(\"y_train \", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Vectorization](https://www.researchgate.net/profile/Aliaa_Rassem/publication/317954962/figure/download/fig2/AS:667792667860996@1536225587611/RNN-simple-cell-versus-LSTM-cell-4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the RNN model\n",
    "\n",
    "if train_model is True:\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(16, kernel_regularizer=regularizers.l1(0.001), activation='relu', input_shape=(10000,)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(16, kernel_regularizer=regularizers.l1(0.001),activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # making the model aware of multiple GPUs\n",
    "    \n",
    "    if n_gpus > 1:\n",
    "        final_model = multi_gpu_model(model, gpus=8)\n",
    "    else:\n",
    "        final_model = model\n",
    "\n",
    "    # compile the model\n",
    "\n",
    "    final_model.compile(optimizer='rmsprop', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # summarize the model\n",
    "    final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "\n",
    "if train_model is True:\n",
    "    n_epochs = 20\n",
    "    batch_size = 512\n",
    "\n",
    "    history = final_model.fit(X_train, \n",
    "                              y_train, \n",
    "                              epochs=n_epochs, \n",
    "                              batch_size=batch_size,\n",
    "                              validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model details\n",
    "\n",
    "if train_model is True:\n",
    "    \n",
    "    # save the model\n",
    "    model.save('rnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "\n",
    "if train_model is True:\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "\n",
    "if train_model is True:\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model\n",
    "\n",
    "if train_model is True:\n",
    "    results = final_model.evaluate(X_test, y_test)\n",
    "    print(\"_\"*50)\n",
    "    print(\"Test Loss and Accuracy\")\n",
    "    print(\"results \", results)\n",
    "    history_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model accuracy\n",
    "\n",
    "if train_model is not True:\n",
    "\n",
    "    from keras.models import load_model\n",
    "    final_model = load_model('rnn_model.h5')\n",
    "    final_model.compile(optimizer='adam',\n",
    "                        loss='categorical_crossentropy', \n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "predictions = final_model.predict(X_test)\n",
    "predictions = (predictions > 0.5)\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "classNames = ['Negative','Positive']\n",
    "plt.title('IMDB reviews sentiment analysis')\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Time Series Forecasting\n",
    "\n",
    "To avoid memory and/or cpu usage issues, it is important to reset the Jupyter Notebook kernel.\n",
    "\n",
    "This task can be performed as:\n",
    "\n",
    "- go to the Jupyter notebook menu (up there)\n",
    "- click on 'Kernel'\n",
    "- click on 'Restart'\n",
    "- wait for the kernel to restart\n",
    "\n",
    "Once the restart procedure is finished, go ahead on the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import read_csv, DataFrame, concat\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.utils.training_utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configuration -- number of GPUs and training option (Yes or No)\n",
    "\n",
    "n_gpus = 1 # knob to make the model parallel or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and process data\n",
    "\n",
    "def parse(x):\n",
    "    return datetime.strptime(x, '%Y %m %d %H')\n",
    "\n",
    "dataset = read_csv('pollution.csv',  \n",
    "                   parse_dates = [['year', 'month', 'day', 'hour']], \n",
    "                   index_col=0, \n",
    "                   date_parser=parse)\n",
    "\n",
    "dataset.drop('No', axis=1, inplace=True)\n",
    "dataset.columns = ['pollution', 'dew', 'temp', 'press', \n",
    "                   'wnd_dir', 'wnd_spd', 'snow', 'rain']\n",
    "dataset.index.name = 'date'\n",
    "dataset['pollution'].fillna(0, inplace=True)\n",
    "dataset = dataset[24:]\n",
    "\n",
    "print('-'*100)\n",
    "print(dataset.head(5))\n",
    "print('-'*100)\n",
    "dataset.to_csv('pollution_parsed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the dataset\n",
    "\n",
    "dataset = read_csv('pollution_parsed.csv', header=0, index_col=0)\n",
    "values = dataset.values\n",
    "groups = [0, 1, 2, 3, 5, 6, 7]\n",
    "i = 1\n",
    "plt.figure()\n",
    "for group in groups:\n",
    "    plt.subplot(len(groups), 1, i)\n",
    "    plt.plot(values[:, group],'k')\n",
    "    plt.title(dataset.columns[group], y=0.5, loc='right')\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "\n",
    "def organize_series(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    \n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    \n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "\n",
    "    return agg\n",
    "\n",
    "# processing the dataset\n",
    "\n",
    "dataset = read_csv('pollution_parsed.csv', header=0, index_col=0)\n",
    "values = dataset.values\n",
    "encoder = LabelEncoder()\n",
    "values[:,4] = encoder.fit_transform(values[:,4])\n",
    "values = values.astype('float32')\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "reframed = organize_series(scaled, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing\n",
    "\n",
    "values = reframed.values\n",
    "n_train_hours = 365 * 24\n",
    "test = values[:n_train_hours, :]\n",
    "train = values[n_train_hours:, :]\n",
    "\n",
    "# split into input and outputs\n",
    "\n",
    "X_train, y_train = train[:, :-1], train[:, -1]\n",
    "X_test, y_test = test[:, :-1], test[:, -1]\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "print(\" Training data shape X, y => \",X_train.shape, y_train.shape,\" Testing data shape X, y => \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the RNN/LSTM model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "# make the model aware of multi GPU\n",
    "\n",
    "if n_gpus > 1:\n",
    "    final_model = multi_gpu_model(model, gpus=8)\n",
    "else:\n",
    "    final_model = model\n",
    "\n",
    "# compile the model\n",
    "final_model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "# summarize the model\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the RNN/LSTM model\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 72\n",
    "\n",
    "history = final_model.fit(X_train, \n",
    "                    y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the loss during training\n",
    "\n",
    "plt.plot(history.history['loss'], 'b', label='Training')\n",
    "plt.plot(history.history['val_loss'],  'r',label='Validation')\n",
    "plt.title(\"Train and Test Loss for the LSTM\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning things up\n",
    "\n",
    "Not much actions must be taken to clean the environment used on this lab.\n",
    "\n",
    "As a new EC2 instance was created for this purpose, simply terminate the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
